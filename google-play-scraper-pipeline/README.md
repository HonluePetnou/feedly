# üì± Feedly - Backend API & Scraper Pipeline

Ce projet constitue le c≈ìur du backend de la plateforme Feedly. Il g√®re l'ingestion des avis Google Play Store, le stockage en base de donn√©es, et l'interface d'intelligence artificielle g√©n√©rative (RAG).

---

## üöÄ Fonctionnalit√©s Cl√©s

* **Scraping Hybride :**
    * **Synchrone :** R√©cup√©ration imm√©diate des 50 premiers avis pour un affichage rapide.
    * **Asynchrone (Background) :** Utilisation de **Celery + Redis** pour scraper l'historique complet (20k+ avis) sans bloquer l'API.

* **Base de Donn√©es :**
    * Gestion des doublons.
    * Nettoyage automatique des donn√©es via PostgreSQL.

* **Chatbot RAG (Retrieval-Augmented Generation) :**
    * Int√©gration de **Google Gemini**.
    * R√©ponse aux questions en langage naturel bas√©es sur les avis stock√©s.

* **API RESTful :**
    * Expos√©e via **FastAPI** pour la communication avec le Frontend.

---

## üõ†Ô∏è Stack Technique

| Composant | Technologie |
| :--- | :--- |
| **Langage** | Python 3.10+ |
| **API Framework** | FastAPI + Uvicorn |
| **Queue & Broker** | Celery + Redis |
| **Database ORM** | SQLAlchemy (PostgreSQL) |
| **AI Provider** | Google Generative AI (Gemini) |

---

## ‚öôÔ∏è Installation & Configuration

### 1. Pr√©requis
* PostgreSQL install√© et fonctionnel.
* Redis install√© et fonctionnel (Service Windows ou Docker).
* Une cl√© API Google Gemini (AI Studio).

### 2. Installation des d√©pendances

```bash
cd google-play-scraper-pipeline
python -m venv venv

# Windows :
.\venv\Scripts\Activate.ps1

pip install -r requirements.txt
3. Configuration (.env)
Cr√©ez un fichier .env √† la racine du dossier avec les variables suivantes :

Ini, TOML

# Base de donn√©es
DB_HOST=localhost
DB_NAME=reviews_db
DB_USER=postgres
DB_PASSWORD=votre_mot_de_passe

# Redis (Broker pour Celery)
CELERY_BROKER_URL=redis://localhost:6379/0

# Intelligence Artificielle (Gemini)
GEMINI_API_KEY=AIzaSyDxxxxxxxxxxxxxxxxxxxxxxxxxx
üèÉ‚Äç‚ôÇÔ∏è D√©marrage des Services
Pour que le backend soit 100% fonctionnel, deux terminaux doivent tourner en parall√®le.

Terminal 1 : L'API (Serveur Web)
C'est le point d'entr√©e pour le Frontend.

Bash

uvicorn src.api:app --reload
Accessible sur : http://localhost:8000

Documentation Swagger : http://localhost:8000/docs

Terminal 2 : Le Worker (T√¢ches de fond)
C'est lui qui effectue le scraping de masse en arri√®re-plan.

Bash

celery -A src.tasks worker --loglevel=info --pool=solo
üîå Documentation API (Endpoints Principaux)
Voici les endpoints cl√©s √† int√©grer dans l'interface utilisateur.

1. Ajouter une Application (POST /add-app)
Lance le scraping. R√©pond imm√©diatement "Accepted" et d√©l√®gue le travail √† Celery.

Payload :

JSON

{
  "app_id": "com.instagram.android",
  "country": "fr",
  "count": 2000
}
R√©ponse : 200 OK avec un task_id pour le suivi.

2. Discuter avec les Donn√©es (POST /chat)
Endpoint pour le Chatbot Intelligent. Il analyse les 50 derniers avis pertinents pour r√©pondre.

Payload :

JSON

{
  "app_id": "com.instagram.android",
  "question": "Quels sont les bugs signal√©s cette semaine ?"
}
R√©ponse :

JSON

{
  "app": "com.instagram.android",
  "response": "Les utilisateurs signalent principalement des crashs au d√©marrage...",
  "analyzed_reviews_count": 50
}


Backend Feedly - 2025