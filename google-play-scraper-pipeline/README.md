ğŸ“± Plateforme de Surveillance Google Play (Backend)
Ce projet est le moteur backend d'une plateforme d'aide Ã  la dÃ©cision basÃ©e sur les avis utilisateurs. Il permet de collecter, nettoyer et stocker les donnÃ©es du Google Play Store via une architecture hybride (Temps rÃ©el + ArriÃ¨re-plan).

ğŸš€ FonctionnalitÃ©s ClÃ©s
EntrÃ©e Intelligente : Accepte une URL Google Play ou un ID d'application (ex: com.whatsapp).

On-Boarding Temps RÃ©el : Scrape un Ã©chantillon immÃ©diat pour valider l'ajout et rÃ©pondre Ã  l'interface en < 3 secondes.

Surveillance ArriÃ¨re-plan : Utilise Celery & Redis pour scraper l'historique massif (milliers d'avis) sans bloquer l'utilisateur.

Pipeline ETL :

Extract : google-play-scraper

Transform : Module de nettoyage automatique (suppression tags traduction, espaces, avis vides).

Load : Stockage structurÃ© dans PostgreSQL.

API REST : ExposÃ©e via FastAPI pour la communication avec le Frontend (React).

ğŸ“‚ Architecture du Projet
Plaintext

google-play-scraper-pipeline/
â”œâ”€â”€ config/               # Fichiers de configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py            # POINT D'ENTRÃ‰E : API FastAPI
â”‚   â”œâ”€â”€ tasks.py          # WORKER : TÃ¢ches d'arriÃ¨re-plan (Celery)
â”‚   â”œâ”€â”€ main_pipeline.py  # Script de test manuel
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ models.py     # SchÃ©ma de la BDD (Applications, Reviews)
â”‚   â”‚   â””â”€â”€ db_manager.py # Connexion PostgreSQL
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ cleaner.py    # Logique de Nettoyage des donnÃ©es
â”‚   â”‚   â””â”€â”€ loader.py     # Logique d'Insertion en BDD
â”‚   â””â”€â”€ scraper/
â”‚       â””â”€â”€ scraper_module.py # Moteur de scraping
â”œâ”€â”€ .env                  # Variables d'environnement (Secrets)
â”œâ”€â”€ requirements.txt      # Liste des dÃ©pendances
â””â”€â”€ README.md             # Documentation
ğŸ› ï¸ Installation & Configuration
1. PrÃ©requis
Python 3.9+

PostgreSQL

Redis (Requis pour les tÃ¢ches d'arriÃ¨re-plan)

2. Installation
Bash

# 1. CrÃ©er et activer l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\Activate.ps1

# 2. Installer les dÃ©pendances
pip install -r requirements.txt
3. Configuration Base de DonnÃ©es
CrÃ©ez un fichier .env Ã  la racine :

Ini, TOML

DB_HOST=localhost
DB_NAME=reviews_db
DB_USER=postgres
DB_PASSWORD=votre_mot_de_passe
CELERY_BROKER_URL=redis://localhost:6379/0
Initialisez les tables :

Bash

python -m src.database.db_manager
ğŸƒâ€â™‚ï¸ DÃ©marrage du SystÃ¨me
Le systÃ¨me nÃ©cessite deux terminaux ouverts simultanÃ©ment.

Terminal 1 : Lancer l'API (Serveur Web)
C'est le point d'entrÃ©e pour le Frontend.

Bash

uvicorn src.api:app --reload
L'API sera accessible sur : http://127.0.0.1:8000

Terminal 2 : Lancer le Worker (ArriÃ¨re-plan)
C'est lui qui traite l'historique et les tÃ¢ches lourdes.

Bash

# Sur Windows (Important : --pool=solo)
celery -A src.tasks worker --loglevel=info --pool=solo

# Sur Linux/Mac
celery -A src.tasks worker --loglevel=info
ğŸ”Œ Documentation de l'API
1. Ajouter une Application (Point d'EntrÃ©e Principal)
UtilisÃ© par le bouton "Chercher" de l'interface utilisateur.

URL : POST /add-app

Description : Lance le scraping immÃ©diat + planifie le scraping complet.

Format JSON :

JSON

{
  "query": "https://play.google.com/store/apps/details?id=com.whatsapp"
}
(Le champ query accepte aussi directement l'ID : com.whatsapp)

2. Lire les Avis (Dashboard)
UtilisÃ© pour afficher les donnÃ©es.

URL : GET /get-reviews/{app_id}

Exemple : /get-reviews/com.whatsapp?limit=100

ğŸ§ª Tests
Vous pouvez tester l'API directement via l'interface Swagger gÃ©nÃ©rÃ©e automatiquement : ğŸ‘‰ http://127.0.0.1:8000/docs